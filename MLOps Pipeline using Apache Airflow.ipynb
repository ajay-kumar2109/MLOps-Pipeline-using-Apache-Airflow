{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make sure airflow installed\n",
    "#pip install apache-airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s start building an MLOps pipeline using Apache Airflow with the necessary data preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date               0\n",
      "App                0\n",
      "Usage (minutes)    0\n",
      "Notifications      0\n",
      "Times Opened       0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load the dataset\n",
    "data = pd.read_csv('C://Users//ajays//Downloads//screentime_analysis.csv')\n",
    "\n",
    "# check for missing values and duplicates\n",
    "print(data.isnull().sum())\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# convert Date column to datetime and extract features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "data['Month'] = data['Date'].dt.month\n",
    "\n",
    "# encode the categorical 'App' column using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['App'], drop_first=True)\n",
    "\n",
    "# scale numerical features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data[['Notifications', 'Times Opened']] = scaler.fit_transform(data[['Notifications', 'Times Opened']])\n",
    "\n",
    "# feature engineering\n",
    "data['Previous_Day_Usage'] = data['Usage (minutes)'].shift(1)\n",
    "data['Notifications_x_TimesOpened'] = data['Notifications'] * data['Times Opened']\n",
    "\n",
    "# save the preprocessed data to a file\n",
    "data.to_csv('preprocessed_screentime_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after preprocessing, we will train a Random Forest model to predict app usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Date                           0\n",
      "Usage (minutes)                0\n",
      "Notifications                  0\n",
      "Times Opened                   0\n",
      "DayOfWeek                      0\n",
      "Month                          0\n",
      "App_Facebook                   0\n",
      "App_Instagram                  0\n",
      "App_LinkedIn                   0\n",
      "App_Netflix                    0\n",
      "App_Safari                     0\n",
      "App_WhatsApp                   0\n",
      "App_X                          0\n",
      "Previous_Day_Usage             1\n",
      "Notifications_x_TimesOpened    0\n",
      "dtype: int64\n",
      "Infinite values in each numeric column:\n",
      "Usage (minutes)                0\n",
      "Notifications                  0\n",
      "Times Opened                   0\n",
      "DayOfWeek                      0\n",
      "Month                          0\n",
      "App_Facebook                   0\n",
      "App_Instagram                  0\n",
      "App_LinkedIn                   0\n",
      "App_Netflix                    0\n",
      "App_Safari                     0\n",
      "App_WhatsApp                   0\n",
      "App_X                          0\n",
      "Previous_Day_Usage             0\n",
      "Notifications_x_TimesOpened    0\n",
      "dtype: int64\n",
      "Mean Absolute Error: 15.534250000000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "\n",
    "# Step 1: Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Step 2: Handle missing values by filling them with the mean of each column\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "# Step 3: Select only numeric columns\n",
    "# Alternative way to select numeric columns\n",
    "numeric_data = data._get_numeric_data()\n",
    "\n",
    "# Step 4: Check for infinite values in numeric columns\n",
    "print(\"Infinite values in each numeric column:\")\n",
    "print(np.isinf(numeric_data).sum())\n",
    "\n",
    "# Step 5: Handle infinite values by replacing them with NaN and then filling with the mean\n",
    "data[numeric_data.columns] = numeric_data.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "# Step 6: Split data into features (X) and target variable (y)\n",
    "# Ensure 'Usage (minutes)' and 'Date' are valid column names in your dataset\n",
    "X = data.drop(columns=['Usage (minutes)', 'Date'])\n",
    "y = data['Usage (minutes)']\n",
    "\n",
    "# Step 7: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Train the RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the data preprocessing function\n",
    "def preprocess_data():\n",
    "    # Load the data\n",
    "    file_path = 'screentime_analysis.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'Date' column to datetime and extract features\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "\n",
    "    # Drop the original 'Date' column\n",
    "    data = data.drop(columns=['Date'])\n",
    "\n",
    "    # One-hot encode the 'App' column\n",
    "    data = pd.get_dummies(data, columns=['App'], drop_first=True)\n",
    "\n",
    "    # Normalize 'Notifications' and 'Times Opened' columns\n",
    "    scaler = MinMaxScaler()\n",
    "    data[['Notifications', 'Times Opened']] = scaler.fit_transform(data[['Notifications', 'Times Opened']])\n",
    "\n",
    "    # Save the preprocessed data to a new file\n",
    "    preprocessed_path = 'preprocessed_screentime_analysis.csv'\n",
    "    data.to_csv(preprocessed_path, index=False)\n",
    "    print(f\"Preprocessed data saved to {preprocessed_path}\")\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    dag_id='data_preprocessing',\n",
    "    schedule='@daily',  # Use 'schedule' instead of 'schedule_interval'\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    catchup=False,\n",
    "    description='A DAG for preprocessing screentime analysis data',\n",
    ")\n",
    "\n",
    "# Define the task\n",
    "preprocess_task = PythonOperator(\n",
    "    task_id='preprocess',\n",
    "    python_callable=preprocess_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Set task dependencies (if you add more tasks later)\n",
    "# preprocess_task >> another_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and Running the Pipeline\n",
    "Testing and Running the Pipeline are meant to be executed in the terminal. These commands should be run in separate terminal windows (or tabs) while your Python environment is active. First, initialize the database:\n",
    "\n",
    "airflow db init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command initializes the metadata database used by Airflow to store details about tasks, DAGs, and schedules.\n",
    "\n",
    "Next, start the Airflow webserver:\n",
    "\n",
    "airflow webserver --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starts the Airflow webserver, which hosts the user interface for managing DAGs and monitoring task execution. The default port is 8080, but you can specify a different port if needed.\n",
    "\n",
    "Finally, start the Airflow scheduler:\n",
    "\n",
    "airflow scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scheduler is responsible for executing tasks as per the DAG schedule. It monitors the tasks and ensures they are executed in the correct order.\n",
    "\n",
    "To access the Airflow UI, navigate to http://localhost:8080 in your browser. Once there, enable the data_preprocessing DAG and manually trigger it to execute the defined tasks. After the DAG has run successfully, validate the output by checking the preprocessed file to ensure it contains the updated and preprocessed data.\n",
    "\n",
    "Summary\n",
    "So, building an MLOps pipeline using Apache Airflow simplifies the end-to-end process of data preprocessing, model training, and deployment. Automating tasks through DAGs ensures efficiency, scalability, and reproducibility in managing machine learning workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
